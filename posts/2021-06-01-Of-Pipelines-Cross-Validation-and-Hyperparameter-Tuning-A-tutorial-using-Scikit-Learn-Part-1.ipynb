{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /2021/06/01/Of-Pipelines-Cross-Validation-and-Hyperparameter-Tuning-A-tutorial-using-Scikit-Learn-Part-1\n",
    "date: '2021-06-01'\n",
    "description: Divine Saungweme explores pipelines, cv and hyperparameter tuning in\n",
    "  SKLearn.\n",
    "output-file: 2021-06-01-of-pipelines-cross-validation-and-hyperparameter-tuning-a-tutorial-using-scikit-learn-part-1.html\n",
    "title: Of Pipelines, Cross Validation and Hyperparameter Tuning - A tutorial using\n",
    "  Scikit-Learn Part 1\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello, Data Scientists\n",
    "\n",
    "In this episode, I'll be demonstrating how powerful some 3 Sci-Kit-Learn features are and how we can use these features to prepare data, to choose models and to fine-tune the model without breaking any sweat because of the simplicity and automation.\n",
    "\n",
    "The features are:\n",
    "\t- Transformation Pipelines\n",
    "\t- Cross Validation\n",
    "\t- And last but not least, Hyperparameter Tuning with Grid Search and Randomized Search\n",
    "\n",
    "We will discover how these features can help us and why they are really worth putting in your Data science tool-kit.\n",
    "\n",
    "We wil be using the **Titanic dataset** from Kaggle Competitions (Hope this episode won't be a disaster too like the Titanic :)\n",
    "\n",
    "Before we begin, if you are using a backward version of **Sci-Kit-Learn** you may have problems in importing some packages. In this tutorial, I am using version **'0.21.3'**.\n",
    "\n",
    "You can use the following piece of code to see the version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)  # To get a stable output across all runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Titanic dataset, we have to create a model that predicts which passengers survived the Titanic shipwreck.\n",
    "We have to predict what sort of people were likely survive using the Passenger information e.g name, gender, passenger class, etc..... \n",
    "So, now let's load the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has already been split into Training and Testing data\n",
    "\n",
    "Since this is a Kaggle Competition dataset, there are no labels in the Test data (with the attribute name 'Survived'). We will just compile our predictions into a csv file (in respect of Kaggles' formating) , upload the predictions (as a csv file) to Kaggle and see our final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some insight about the data**\n",
    "\n",
    "The attributes have the following meaning:\n",
    "* **Survived**: that's the target, 0 means the passenger did not survive, while 1 means he/she survived.\n",
    "* **Pclass**: passenger class.\n",
    "* **Name**, **Sex**, **Age**: self-explanatory\n",
    "* **SibSp**: how many siblings & spouses of the passenger aboard the Titanic.\n",
    "* **Parch**: how many children & parents of the passenger aboard the Titanic.\n",
    "* **Ticket**: ticket id\n",
    "* **Fare**: price paid (in pounds)\n",
    "* **Cabin**: passenger's cabin number\n",
    "* **Embarked**: where the passenger embarked the Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for any missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the attributes: **Age, Cabin and Embarked** have some null values and **Cabin** has the most null values.\n",
    "\n",
    "We will not be using **Cabin** in this tutorial and we will also not use the **Name** and **Ticket** attributes.\n",
    "\n",
    "We can easily use the **Age** and the **Embarked** attributes so we will transform them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let take a sneak peak at the numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let also take a sneak peak at the categorial attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    549\n",
       "1    342\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    491\n",
       "1    216\n",
       "2    184\n",
       "Name: Pclass, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Pclass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male      577\n",
       "female    314\n",
       "Name: Sex, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S    644\n",
       "C    168\n",
       "Q     77\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embarked attribute tells us where the passenger embarked: C=Cherbourg, Q=Queenstown, S=Southampton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "# Use the \"copy()\" function call to avoid changing the original data (which in this case is \"train_data\")\n",
    "# We have to poke around with transformations using the copy of train_data and see what we can archieve\n",
    "\n",
    "train_data_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start using Transformations.\n",
    "\n",
    "Using the \"so-called\" regular way we would need to deal with the NaN values first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Inspired from stackoverflow.com/questions/25239958\n",
    "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n",
    "                                        index=X.columns)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.most_frequent_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class transforms both numerical and categorial (object) data. It replaces the NaN values in the data with the most frequent value in the Data Attribute.\n",
    "For example, If we had NaN values in the **Sex attribute**, we would replace the NaN values with the most frequent value in the **Sex attribute** (which in this case is **Male**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use our Transformer Class (MostFrequentImputer) on categorial attributes. We will use **Simple Imputer** for numerical attributes.\n",
    "Let's start with numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "impute = SimpleImputer(strategy='median')\n",
    "# Setting strategy to median indicates that we want to convert the NaN values to the median of the numerical attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            891 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "impute.fit(train_data_copy[[\"Age\"]])\n",
    "train_data_copy[[\"Age\"]] = impute.transform(train_data_copy[[\"Age\"]])\n",
    "train_data_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the values from our imputed attribute (Age) are now 891, the NaN values in the attribute have been wiped off from existance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's impute the categorial attributes with our Transform Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            891 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       891 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "cat_impute = MostFrequentImputer()\n",
    "cat_impute.fit(train_data_copy[[\"Embarked\"]])\n",
    "train_data_copy[[\"Embarked\"]] = cat_impute.transform(train_data_copy[[\"Embarked\"]])\n",
    "train_data_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, what do you know?. Now, only the **Cabin** attribute is left (because we didn't include it in our transformation). We are not going to use it along with **Name** and **Ticket**, so let's just drop these attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy.drop(['Name', 'Cabin', 'Ticket'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew!!, we have finally got rid of the NaN values so what's next. We want to scale the Numerical Attributes (some algorithms work better with scaled data). For categories we have to convert the categories from strings to usable numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>male</td>\n",
       "      <td>-0.565736</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.502445</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.566107</td>\n",
       "      <td>female</td>\n",
       "      <td>0.663861</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.786845</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>female</td>\n",
       "      <td>-0.258337</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.488854</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.566107</td>\n",
       "      <td>female</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.420730</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>male</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.486337</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived    Pclass     Sex       Age     SibSp     Parch  \\\n",
       "0            1         0  0.827377    male -0.565736  0.432793 -0.473674   \n",
       "1            2         1 -1.566107  female  0.663861  0.432793 -0.473674   \n",
       "2            3         1  0.827377  female -0.258337 -0.474545 -0.473674   \n",
       "3            4         1 -1.566107  female  0.433312  0.432793 -0.473674   \n",
       "4            5         0  0.827377    male  0.433312 -0.474545 -0.473674   \n",
       "\n",
       "       Fare Embarked  \n",
       "0 -0.502445        S  \n",
       "1  0.786845        C  \n",
       "2 -0.488854        S  \n",
       "3  0.420730        S  \n",
       "4 -0.486337        S  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_attribs = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Pclass\"]\n",
    "# Pclass is a category but it's already in numerical so scaling it would be a good idea than labeling it in this case\n",
    "scaler = StandardScaler()\n",
    "train_data_copy[num_attribs] = scaler.fit_transform(train_data_copy[num_attribs])\n",
    "train_data_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected attributes have been scaled, Now our models can predict much better, Now what's left is tagging numerical labels to our Categorial attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labeler = LabelEncoder()\n",
    "for cats in [\"Sex\", \"Embarked\"]: # cats is just short for categories, not actual cats ;)\n",
    "    train_data_copy[cats] = labeler.fit_transform(train_data_copy[cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.565736</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.502445</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.566107</td>\n",
       "      <td>0</td>\n",
       "      <td>0.663861</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.786845</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.258337</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.488854</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.566107</td>\n",
       "      <td>0</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.420730</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>1</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.486337</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived    Pclass  Sex       Age     SibSp     Parch  \\\n",
       "0            1         0  0.827377    1 -0.565736  0.432793 -0.473674   \n",
       "1            2         1 -1.566107    0  0.663861  0.432793 -0.473674   \n",
       "2            3         1  0.827377    0 -0.258337 -0.474545 -0.473674   \n",
       "3            4         1 -1.566107    0  0.433312  0.432793 -0.473674   \n",
       "4            5         0  0.827377    1  0.433312 -0.474545 -0.473674   \n",
       "\n",
       "       Fare  Embarked  \n",
       "0 -0.502445         2  \n",
       "1  0.786845         0  \n",
       "2 -0.488854         2  \n",
       "3  0.420730         2  \n",
       "4 -0.486337         2  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have finished preparing our data. What a lot of tiring work that was, Good news, you don't have to tire yourself with this tedious technique.\n",
    "\n",
    "**Pipelines coming to the rescue...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why should I care about **Pipelines** ????\n",
    "\n",
    "As we have seen (and coded as well), we have many transformations that need to be executed in the right order, for example: We cannot scale data whilst we still have NaN values in the data (You end up getting many frustrating errors).\n",
    "\n",
    "Pipelines take all the transformations and bind them together inorder to prepare/transform the data in the right order. All we should do is specify the order of executions by putting the transformation packages in the right order.\n",
    "\n",
    "So how do go about setting up these so-called Pipelines, let's dive right into the Pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline # how ironic, importing Pipeline from pipeline ;)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# In this one we will use OneHotEncoder instead of LabelEncoder as OneHotEncoder tends to do a better job than LabelEncoder\n",
    "\n",
    "# We will make 2 Pipelines, one for Numerical Attributes and the other for Categorial Attributes\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    # The name \"imputer\" can be set to any string e.g \"impute\" or \"whatever\"\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# I had already imported these packages before so there's no need for repetition\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', MostFrequentImputer()), # from the class we had made earlier\n",
    "    ('encoding', OneHotEncoder(sparse=False))\n",
    "    # Setting sparse to False prevents the OneHotEncoder from returning a Scipy sparse matrix\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is even a much faster way of setting up a Pipeline. Sometimes we may have no need of naming each step taken in preprocessing. Using \"**make_pipeline**\", we can save time although we would sacrifice some features a general Pipeline offers. You can use the following code below to set up the faster-to-setup Pipeline (although we are not going to use it, we will just stick to the general pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "num_small_pipeline = make_pipeline(SimpleImputer(strategy='median'), StandardScaler())\n",
    "cat_small_pipeline = make_pipeline(MostFrequentImputer(), OneHotEncoder(sparse=False))\n",
    "\n",
    "# It's easy as that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about **OneHotEncoder**\n",
    "\n",
    "A OneHotEncoder creates binary columns (attributes) from the category\n",
    "\n",
    "What does that mean??\n",
    "\n",
    "For example: The **Embarked** category. \n",
    "The attributes in the category **Embarked** are: S; C; and Q.\n",
    "\n",
    "We get 3 columns for this category (because it has 3 attributes stated earlier)\n",
    "\n",
    "If the passenger embarked from S, the S column will have 1 (making it hot and also representing True) and the other columns (C and Q) will have 0 (making them cold and also representing False)\n",
    "\n",
    "If the passenger embarked from C, the C column will have 1 (making it hot and also representing True) and the other columns (S and Q) will have 0 (making them cold and also representing False)\n",
    "\n",
    ".....\n",
    "\n",
    "So we will have extra columns.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we must seperate the train_data into 'Data' and 'Target'\n",
    "# Only the 'Data' needs to be transformed with our Pipeline\n",
    "\n",
    "train_set = train_data.drop('Survived', axis=1)\n",
    "y_train = train_data['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have made our Pipelines, we will combine them with ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.56573646,  0.43279337, -0.47367361, -0.50244517,  0.        ,\n",
       "         0.        ,  1.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.66386103,  0.43279337, -0.47367361,  0.78684529,  1.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.25833709, -0.4745452 , -0.47367361, -0.48885426,  0.        ,\n",
       "         0.        ,  1.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.4333115 ,  0.43279337, -0.47367361,  0.42073024,  1.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.4333115 , -0.4745452 , -0.47367361, -0.48633742,  0.        ,\n",
       "         0.        ,  1.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  1.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num_pipeline', num_pipeline, [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]),\n",
    "    # We just choose the numerical attributes we want\n",
    "    ('cat_pipeline', cat_pipeline, [\"Pclass\", \"Sex\", \"Embarked\"])\n",
    "    # Here, we just choose the categorial attributes we want and here Pclass works better as a category than a number\n",
    "])\n",
    "\n",
    "X_train = full_pipeline.fit_transform(train_set)\n",
    "\n",
    "# The transformation returns our data as a numpy array\n",
    "# Only the attributes of numbers and categories that we have specified in the full_pipeline (which are just 7) will\n",
    "# be present in the data so there is really no need of dropping attributes like we did before in our \"Regular\n",
    "# Transformation\" detour because they have been automatically dropped.\n",
    "\n",
    "X_train[:5] # Our numpy array's head, Similar with Pandas .head() function call :)\n",
    "# Voila..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass-1</th>\n",
       "      <th>Pclass-2</th>\n",
       "      <th>Pclass-3</th>\n",
       "      <th>Sex-Female</th>\n",
       "      <th>Sex-Male</th>\n",
       "      <th>Embarked-C</th>\n",
       "      <th>Embarked-Q</th>\n",
       "      <th>Embarked-S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.565736</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.502445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.663861</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.786845</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.258337</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.488854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.420730</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.486337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age     SibSp     Parch      Fare  Pclass-1  Pclass-2  Pclass-3  \\\n",
       "0 -0.565736  0.432793 -0.473674 -0.502445       0.0       0.0       1.0   \n",
       "1  0.663861  0.432793 -0.473674  0.786845       1.0       0.0       0.0   \n",
       "2 -0.258337 -0.474545 -0.473674 -0.488854       0.0       0.0       1.0   \n",
       "3  0.433312  0.432793 -0.473674  0.420730       1.0       0.0       0.0   \n",
       "4  0.433312 -0.474545 -0.473674 -0.486337       0.0       0.0       1.0   \n",
       "\n",
       "   Sex-Female  Sex-Male  Embarked-C  Embarked-Q  Embarked-S  \n",
       "0         0.0       1.0         0.0         0.0         1.0  \n",
       "1         1.0       0.0         1.0         0.0         0.0  \n",
       "2         1.0       0.0         0.0         0.0         1.0  \n",
       "3         1.0       0.0         0.0         0.0         1.0  \n",
       "4         0.0       1.0         0.0         0.0         1.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As you saw, Pipelines are easily managable than the whole transformation process we had earlier and Pipelines also\n",
    "# take less time to set up\n",
    "\n",
    "# Let's see how our data looks like as a DataFrame\n",
    "\n",
    "# Pclass: 1, 2, 3\n",
    "# Sex: Female, Male\n",
    "# Embarked: S,C, Q\n",
    "\n",
    "columns = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", 'Pclass-1', 'Pclass-2', 'Pclass-3', \n",
    "           'Sex-Female', 'Sex-Male', 'Embarked-C', 'Embarked-Q', 'Embarked-S']\n",
    "\n",
    "pd.DataFrame(X_train, columns=columns).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will start by using the Stochastic Gradient Descent Classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = full_pipeline.transform(test_data)\n",
    "predictions = sgd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We convert our predictions to a csv file\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.to_csv('predictions.csv')\n",
    "\n",
    "# Then you can submit the csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But, Wait a minute!!......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we get an idea of how our model performs, What if it turns out be very aweful. Luckily we don't have to rely on guess-work.\n",
    "\n",
    "Let's try predicting the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9797979797979798"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dt_clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "dt_pred = dt_clf.predict(X_train)\n",
    "accuracy_score(y_train, dt_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you smile in satisfaction, those numbers are biased. \n",
    "The model is simply overfitting\n",
    "\n",
    "Predicting the train data mostly results in Models overfitting the data (as we have seen), So how can we get a reliable score without worring about overfitting...\n",
    "\n",
    "We can can use Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(score):\n",
    "    print(score)\n",
    "    print(f'\\nMean: {score.mean()}')\n",
    "    print(f'\\nStandard Deviation: {score.std()}')\n",
    "    print(f'\\nMaximum: {score.min()}')\n",
    "    print(f'Minimun: {score.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76666667 0.72222222 0.7752809  0.86516854 0.7752809  0.73033708\n",
      " 0.75280899 0.78651685 0.80898876 0.78409091]\n",
      "\n",
      "Mean: 0.7767361820451708\n",
      "\n",
      "Standard Deviation: 0.03848773108423685\n",
      "\n",
      "Maximum: 0.7222222222222222\n",
      "Minimun: 0.8651685393258427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cvs = cross_val_score(sgd_clf, X_train, y_train, scoring='accuracy', cv=10)\n",
    "\n",
    "scores(cvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what just happened, you may be wondering how we got the score\n",
    "\n",
    "The cross_val_score randomly splits the training set into 10 distinct subsets called folds, then it trains and\n",
    "evaluates the model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores.\n",
    "\n",
    "The \"cv\" parameter determines how many folds we want our cross_val_score function to have (which in this case is **10**).\n",
    "\n",
    "**NB**. The cv should be greater than 1. **cv > 1**\n",
    "\n",
    "The scoring parameter determines what kind of score we want to get, setting it to:\n",
    "    - \"accuracy\" -  gives us the accuracy score\n",
    "    - \"precision\" -  gives us the precision score\n",
    "    - \"recall\" -  gives us the recall score and \n",
    "    - \"f1\" -  gives us the f1 score..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cross_val_score** was adapted from **StratifiedKFold**, this is how it looks when using **StratifiedKFold**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7666666666666667\n",
      "0.7222222222222222\n",
      "0.7752808988764045\n",
      "0.8651685393258427\n",
      "0.7752808988764045\n",
      "0.7303370786516854\n",
      "0.7528089887640449\n",
      "0.7865168539325843\n",
      "0.8089887640449438\n",
      "0.7840909090909091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=10, random_state=42)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = (y_train[train_index])\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = (y_train[test_index])\n",
    "\n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the results, they are pretty much the same, but using **StratifiedKFold** is a lot of work as compared to using **cross_val_score**.\n",
    "\n",
    "Cross validation can be used to see how our model is able generalize data. If you do not have enough data to populate both the train and test sets, you can definitely use Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77665544332211"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And if you would like to get some predictions so that you can compare them with the y_train set you can simply:\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "predictions = cross_val_predict(sgd_clf, X_train, y_train, cv=10)\n",
    "accuracy_score(y_train, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.80887674,  2.61659364, -0.13381379,  1.82152503, -2.71375558])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you would like to get some Decision Functions you can simply:\n",
    "\n",
    "predictions = cross_val_predict(sgd_clf, X_train, y_train, cv=10, method='decision_function')\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using a model that supports \"Prediction Probabilities\" you can simple set the **method** hyperparameter to **\"predict_proba\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8069611848825332"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try another model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=10).fit(X_train, y_train)\n",
    "cross_val_score(knn_clf, X_train, y_train, scoring='accuracy', cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, the KNeighborsClassifier did a better job than the SGDClassifier, so it's promising\n",
    "\n",
    "Let just try one last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8115690614005221"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The other model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42).fit(X_train, y_train)\n",
    "cross_val_score(rnd_clf, X_train, y_train, scoring='accuracy', cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Classifier did better than the other two but how can we optimise it, whilst also preventing overfitting\n",
    "\n",
    "We can tweak the hyperparameters and see which ones get us somewhere..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.7554752581999773\n",
      "2 0.7958001361933945\n",
      "3 0.7980603790716151\n",
      "4 0.8002698331630915\n",
      "5 0.80697366927704\n",
      "6 0.809170355237771\n",
      "7 0.8058631256384066\n",
      "8 0.7991212688684599\n",
      "9 0.7946646237657474\n",
      "10 0.8069611848825332\n",
      "11 0.7980101577573487\n"
     ]
    }
   ],
   "source": [
    "# A very simple way of tweaking the n_neighbors parameter in KNeighborsClassifier is this way:\n",
    "\n",
    "for number in range(1, 12):\n",
    "    knn_looped = KNeighborsClassifier(n_neighbors=number).fit(X_train, y_train)\n",
    "    score = cross_val_score(knn_looped, X_train, y_train, scoring='accuracy', cv=10).mean()\n",
    "    print(number, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we can see, putting the n_neighbors as **6** tends to give us a higher accuracy score\n",
    "\n",
    "But what can we do if we a lot of hyperparameters in the model that we need to test out and we need a much organised way of doing it.\n",
    "\n",
    "We can use GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] max_features=8, n_estimators=100 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. max_features=8, n_estimators=100, total=   0.2s\n",
      "[CV] max_features=8, n_estimators=100 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. max_features=8, n_estimators=100, total=   0.2s\n",
      "[CV] max_features=8, n_estimators=100 ................................\n",
      "[CV] ................. max_features=8, n_estimators=100, total=   0.2s\n",
      "[CV] max_features=8, n_estimators=200 ................................\n",
      "[CV] ................. max_features=8, n_estimators=200, total=   0.4s\n",
      "[CV] max_features=8, n_estimators=200 ................................\n",
      "[CV] ................. max_features=8, n_estimators=200, total=   0.4s\n",
      "[CV] max_features=8, n_estimators=200 ................................\n",
      "[CV] ................. max_features=8, n_estimators=200, total=   0.5s\n",
      "[CV] max_features=8, n_estimators=300 ................................\n",
      "[CV] ................. max_features=8, n_estimators=300, total=   0.7s\n",
      "[CV] max_features=8, n_estimators=300 ................................\n",
      "[CV] ................. max_features=8, n_estimators=300, total=   0.7s\n",
      "[CV] max_features=8, n_estimators=300 ................................\n",
      "[CV] ................. max_features=8, n_estimators=300, total=   0.7s\n",
      "[CV] max_features=9, n_estimators=100 ................................\n",
      "[CV] ................. max_features=9, n_estimators=100, total=   0.2s\n",
      "[CV] max_features=9, n_estimators=100 ................................\n",
      "[CV] ................. max_features=9, n_estimators=100, total=   0.2s\n",
      "[CV] max_features=9, n_estimators=100 ................................\n",
      "[CV] ................. max_features=9, n_estimators=100, total=   0.2s\n",
      "[CV] max_features=9, n_estimators=200 ................................\n",
      "[CV] ................. max_features=9, n_estimators=200, total=   0.5s\n",
      "[CV] max_features=9, n_estimators=200 ................................\n",
      "[CV] ................. max_features=9, n_estimators=200, total=   0.6s\n",
      "[CV] max_features=9, n_estimators=200 ................................\n",
      "[CV] ................. max_features=9, n_estimators=200, total=   0.5s\n",
      "[CV] max_features=9, n_estimators=300 ................................\n",
      "[CV] ................. max_features=9, n_estimators=300, total=   0.7s\n",
      "[CV] max_features=9, n_estimators=300 ................................\n",
      "[CV] ................. max_features=9, n_estimators=300, total=   0.7s\n",
      "[CV] max_features=9, n_estimators=300 ................................\n",
      "[CV] ................. max_features=9, n_estimators=300, total=   0.7s\n",
      "[CV] max_features=10, n_estimators=100 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=100, total=   0.3s\n",
      "[CV] max_features=10, n_estimators=100 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=100, total=   0.2s\n",
      "[CV] max_features=10, n_estimators=100 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=100, total=   0.3s\n",
      "[CV] max_features=10, n_estimators=200 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=200, total=   0.5s\n",
      "[CV] max_features=10, n_estimators=200 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=200, total=   0.6s\n",
      "[CV] max_features=10, n_estimators=200 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=200, total=   0.5s\n",
      "[CV] max_features=10, n_estimators=300 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=300, total=   0.9s\n",
      "[CV] max_features=10, n_estimators=300 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=300, total=   0.8s\n",
      "[CV] max_features=10, n_estimators=300 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=300, total=   0.7s\n",
      "[CV] max_features=11, n_estimators=100 ...............................\n",
      "[CV] ................ max_features=11, n_estimators=100, total=   0.2s\n",
      "[CV] max_features=11, n_estimators=100 ...............................\n",
      "[CV] ................ max_features=11, n_estimators=100, total=   0.2s\n",
      "[CV] max_features=11, n_estimators=100 ...............................\n",
      "[CV] ................ max_features=11, n_estimators=100, total=   0.3s\n",
      "[CV] max_features=11, n_estimators=200 ...............................\n",
      "[CV] ................ max_features=11, n_estimators=200, total=   0.5s\n",
      "[CV] max_features=11, n_estimators=200 ...............................\n",
      "[CV] ................ max_features=11, n_estimators=200, total=   0.6s\n",
      "[CV] max_features=11, n_estimators=200 ...............................\n",
      "[CV] ................ max_features=11, n_estimators=200, total=   0.5s\n",
      "[CV] max_features=11, n_estimators=300 ...............................\n",
      "[CV] ................ max_features=11, n_estimators=300, total=   0.9s\n",
      "[CV] max_features=11, n_estimators=300 ...............................\n",
      "[CV] ................ max_features=11, n_estimators=300, total=   1.0s\n",
      "[CV] max_features=11, n_estimators=300 ...............................\n",
      "[CV] ................ max_features=11, n_estimators=300, total=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:   20.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False, random_state=42,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid=[{'max_features': [8, 9, 10, 11],\n",
       "                          'n_estimators': [100, 200, 300]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = [\n",
    "    {'n_estimators': [100, 200, 300], 'max_features': [8, 9, 10, 11]}\n",
    "]\n",
    "# These are the parameters that we put in Random Forest model and test each and every combination\n",
    "# So we have 12 combinations...\n",
    "\n",
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "grid_rnd_clf = GridSearchCV(rnd_clf, params, cv=3, return_train_score=True, scoring='accuracy', verbose=2)\n",
    "\n",
    "# The grid search takes the algorithm, parameters, folds/cv (number of trainings)\n",
    "\n",
    "# The grid search undergoes cross validation, similar with the cross_val_score that we talked about earlier, it goes on...\n",
    "# ... cross validating each and every combination we assigned it to\n",
    "\n",
    "# So with the number of folds as 3, we can conclude that we will have 36 runs\n",
    "# {(3 n_estimators) * (4 max_features) * (3 cv)} => 3 * 4 * 3 => 36\n",
    "\n",
    "# 'verbose' gives us details of the runs, such as the time taken, etc, increasing the value increases the details...\n",
    "\n",
    "grid_rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "# Fitting our Grid Search Model make take some time, maybe a few seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8092031425364759"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rnd_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 10, 'n_estimators': 200}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the best combinations/parameters\n",
    "grid_rnd_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8024691358024691 {'max_features': 8, 'n_estimators': 100}\n",
      "0.8035914702581369 {'max_features': 8, 'n_estimators': 200}\n",
      "0.8013468013468014 {'max_features': 8, 'n_estimators': 300}\n",
      "0.8013468013468014 {'max_features': 9, 'n_estimators': 100}\n",
      "0.8047138047138047 {'max_features': 9, 'n_estimators': 200}\n",
      "0.8047138047138047 {'max_features': 9, 'n_estimators': 300}\n",
      "0.8058361391694725 {'max_features': 10, 'n_estimators': 100}\n",
      "0.8092031425364759 {'max_features': 10, 'n_estimators': 200}\n",
      "0.8069584736251403 {'max_features': 10, 'n_estimators': 300}\n",
      "0.8058361391694725 {'max_features': 11, 'n_estimators': 100}\n",
      "0.8024691358024691 {'max_features': 11, 'n_estimators': 200}\n",
      "0.8058361391694725 {'max_features': 11, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# Let's see the whole parameters and their scores\n",
    "data = grid_rnd_clf.cv_results_\n",
    "for a, b in zip(data['mean_test_score'], data['params']):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems as if our best Grid Search model is worse than the regular Random Forest Classifier that we made the first time. This is because we have used a cv of **3** on the **Grid Search Random Forest Classifier** and a cv of **10** on the **Regular Random Forest Classifier**. Let's cross_val_score our best model and see how it does with a cv of 10..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8227681307456589"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf = grid_rnd_clf.best_estimator_ # grid_rnd_clf.best_estimator_ is our model\n",
    "cross_val_score(rnd_clf, X_train, y_train, cv=10, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, the **Grid Search Random Forest Classifier** is actually better than the **Regular Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you are curious:\n",
    "\n",
    "**RandomForestClassifier(random_state=42, n_estimators=200, max_features=10)**\n",
    "\n",
    "and \n",
    "\n",
    "**grid_rnd_clf.best_estimator_** \n",
    "\n",
    "Produce very slightly different results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] max_features=9, n_estimators=192 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. max_features=9, n_estimators=192, total=   0.5s\n",
      "[CV] max_features=9, n_estimators=192 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. max_features=9, n_estimators=192, total=   0.5s\n",
      "[CV] max_features=9, n_estimators=192 ................................\n",
      "[CV] ................. max_features=9, n_estimators=192, total=   0.5s\n",
      "[CV] max_features=8, n_estimators=171 ................................\n",
      "[CV] ................. max_features=8, n_estimators=171, total=   0.4s\n",
      "[CV] max_features=8, n_estimators=171 ................................\n",
      "[CV] ................. max_features=8, n_estimators=171, total=   0.4s\n",
      "[CV] max_features=8, n_estimators=171 ................................\n",
      "[CV] ................. max_features=8, n_estimators=171, total=   0.4s\n",
      "[CV] max_features=10, n_estimators=120 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=120, total=   0.3s\n",
      "[CV] max_features=10, n_estimators=120 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=120, total=   0.3s\n",
      "[CV] max_features=10, n_estimators=120 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=120, total=   0.3s\n",
      "[CV] max_features=7, n_estimators=182 ................................\n",
      "[CV] ................. max_features=7, n_estimators=182, total=   0.4s\n",
      "[CV] max_features=7, n_estimators=182 ................................\n",
      "[CV] ................. max_features=7, n_estimators=182, total=   0.4s\n",
      "[CV] max_features=7, n_estimators=182 ................................\n",
      "[CV] ................. max_features=7, n_estimators=182, total=   0.4s\n",
      "[CV] max_features=8, n_estimators=174 ................................\n",
      "[CV] ................. max_features=8, n_estimators=174, total=   0.4s\n",
      "[CV] max_features=8, n_estimators=174 ................................\n",
      "[CV] ................. max_features=8, n_estimators=174, total=   0.4s\n",
      "[CV] max_features=8, n_estimators=174 ................................\n",
      "[CV] ................. max_features=8, n_estimators=174, total=   0.4s\n",
      "[CV] max_features=10, n_estimators=199 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=199, total=   0.5s\n",
      "[CV] max_features=10, n_estimators=199 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=199, total=   0.5s\n",
      "[CV] max_features=10, n_estimators=199 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=199, total=   0.5s\n",
      "[CV] max_features=8, n_estimators=121 ................................\n",
      "[CV] ................. max_features=8, n_estimators=121, total=   0.3s\n",
      "[CV] max_features=8, n_estimators=121 ................................\n",
      "[CV] ................. max_features=8, n_estimators=121, total=   0.3s\n",
      "[CV] max_features=8, n_estimators=121 ................................\n",
      "[CV] ................. max_features=8, n_estimators=121, total=   0.3s\n",
      "[CV] max_features=10, n_estimators=101 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=101, total=   0.4s\n",
      "[CV] max_features=10, n_estimators=101 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=101, total=   0.2s\n",
      "[CV] max_features=10, n_estimators=101 ...............................\n",
      "[CV] ................ max_features=10, n_estimators=101, total=   0.4s\n",
      "[CV] max_features=9, n_estimators=129 ................................\n",
      "[CV] ................. max_features=9, n_estimators=129, total=   0.5s\n",
      "[CV] max_features=9, n_estimators=129 ................................\n",
      "[CV] ................. max_features=9, n_estimators=129, total=   0.3s\n",
      "[CV] max_features=9, n_estimators=129 ................................\n",
      "[CV] ................. max_features=9, n_estimators=129, total=   0.3s\n",
      "[CV] max_features=7, n_estimators=163 ................................\n",
      "[CV] ................. max_features=7, n_estimators=163, total=   0.4s\n",
      "[CV] max_features=7, n_estimators=163 ................................\n",
      "[CV] ................. max_features=7, n_estimators=163, total=   0.4s\n",
      "[CV] max_features=7, n_estimators=163 ................................\n",
      "[CV] ................. max_features=7, n_estimators=163, total=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   11.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_sc...\n",
       "                                                    warm_start=False),\n",
       "                   iid='warn', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000000000BB2F0C8>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000000000BB2FFC8>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {'n_estimators': randint(low=100, high=200), 'max_features': randint(low=6, high=11)}\n",
    "\n",
    "\n",
    "rnd_clf2 = RandomForestClassifier(random_state=42)\n",
    "rnd_search = RandomizedSearchCV(rnd_clf2, param_distributions = param_distribs, n_iter=10, cv=3, scoring='accuracy',\n",
    "                               random_state=42, verbose=2)\n",
    "rnd_search.fit(X_train, y_train)\n",
    "\n",
    "# Too much randomness on this one, Ehh :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________\n",
    "_RandomizedSearchCV_ takes a dictionary with the parameters, it takes random numbers between the low and high rating and picks randomly the combinations.\n",
    "\n",
    "So the difference between GridSearchCV and RandomizedSearchCV is:\n",
    "\n",
    "With **GridSearchCV** you specify the parameters that you want to combine but with **RandomizedSearchCV**, the model automatically picks random parameters between the specified limits (which are **low** and **high**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8103254769921436, {'max_features': 10, 'n_estimators': 120})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search.best_score_, rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8058361391694725 {'max_features': 9, 'n_estimators': 192}\n",
      "0.8002244668911336 {'max_features': 8, 'n_estimators': 171}\n",
      "0.8103254769921436 {'max_features': 10, 'n_estimators': 120}\n",
      "0.8002244668911336 {'max_features': 7, 'n_estimators': 182}\n",
      "0.8013468013468014 {'max_features': 8, 'n_estimators': 174}\n",
      "0.8092031425364759 {'max_features': 10, 'n_estimators': 199}\n",
      "0.8002244668911336 {'max_features': 8, 'n_estimators': 121}\n",
      "0.8058361391694725 {'max_features': 10, 'n_estimators': 101}\n",
      "0.8058361391694725 {'max_features': 9, 'n_estimators': 129}\n",
      "0.8024691358024691 {'max_features': 7, 'n_estimators': 163}\n"
     ]
    }
   ],
   "source": [
    "# Let's see the whole parameters and their scores\n",
    "data = rnd_search.cv_results_\n",
    "for a, b in zip(data['mean_test_score'], data['params']):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824990352967881"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf = rnd_search.best_estimator_ # rnd_search.best_estimator_ is our model\n",
    "cross_val_score(rnd_clf, X_train, y_train, cv=10, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score from the cross_val_score is actually higher than that of GridSearch.\n",
    "\n",
    "The **RandomizedSearchCV** and the **GridSearchCV** are great, each one in its own way.\n",
    "\n",
    "You can try many comprehensive data manipulation techniques (e.g feature engineering) that can increase the accuracy of your model so that you can upload your predictions with a smile.\n",
    "\n",
    "For now, Goodbye!\n",
    "\n",
    "**Coding Is Fun But I've Gotta Run :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
